\documentclass[12pt,letterpaper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{epsfig}
\graphicspath{{./images/}} % Figures path - used in graphicx
\author{Christopher C. Lamb}
\title{Policy Overlay Networks}
\date{\today}
\begin{document}

\maketitle

\doublespacing

\begin{abstract}
Abstract - TBD
\end{abstract}

\section{Introduction}
Current enterprise computing systems are facing a troubling future.  As things stand today, they are too expensive, unreliable, and information dissemination procedures are just too slow.

Generally, such systems still do not use current commercial resources as well as they could and use costly data partitioning schemes.  Most of these kinds of systems use some combination of systems managed in house by the enterprise itself rather than exploiting lower cost cloud-enabled services.  Furthermore, many of these systems have large maintenance loads imposed on them as a result of internal infrastructural requirements like data and database management or systems administration.  In many cases networks containing sensitive data are separated from other internal networks to enhance data security at the expense of productivity, leading to decreased working efficiencies and increased costs.

These kinds of large distributed systems suffer from a lack of stability and reliability as a direct result of their inflated provisioning and support costs.  Simply put, they large cost and effort burden of these systems precludes the ability to implement the appropriate redundancy and fault tolerance in any but the absolutely most critical systems.  Justifying the costs associated with standard reliability practices like diverse entry or geographically separated hot spares is more and more difficult to do unless forced by draconian legal policy or similarly dire business conditions.

Finally, the length of time between when a sensitive document or other type of data artifact is requested and when it can be delivered to a requester with acceptable need to view that artifact is prohibitively long.  These kinds of sensitive artifacts, usually maintained on partitioned networks or systems, require large amounts of review by specially trained reviewers prior to release to data requesters.  In cases where acquisition of this data is under heaving time constraints like sudden market shifts or other unexpected conditional changes this long review time can result in consequences ranging from financial losses to loss of life.

Federal computer systems are prime examples of these kinds of problematic distributed systems, and demonstrate the difficulty inherent in implementing new technical solutions.  They, like other similar systems, need to be re-imagined to take advantage of radical market shifts in computational provisioning.
\section{Motivation}
Current policy-centric systems are being forced to move to cloud environments and build much more open systems.  Some of these environments will be private or hybrid cloud systems, where private clouds are infrastructure that is completely run and operated by an organization for wider use and provisioning, while hybrid clouds are combinations of private and public cloud systems.  Driven by both cost savings and efficiency requirements, this migration will result in a loss of control of computing resources by involved organizations as they attempt to exploit economies of scale and utility computing.

Robust usage management will become an even more important issue in these environments.  Federal organizations poised to benefit from this migration include agencies like the National Security Agency (NSA) and the Department of Defense (DoD), both of whom have large installed bases of compartmentalized and classified data.  The DoD realizes the scope of this effort, understanding that such technical change must incorporate effectively sharing needed data with other federal agencies, foreign governments, and international organizations \cite{proposal:info-sharing-strategy}.  Likewise, the NSA is focused on exploiting cloud-centric systems to facilitate information dissemination and sharing \cite{proposal:nsa-cloud}.

Cloud systems certainly exhibit economic incentives for use, providing cost savings and flexibility but they also have distinct disadvantages as well.  Specifically, the are not intrinsically as private as some current systems, generally can be less secure than department-level solutions, and have the kind of trust issues that therapists cannot adequately address \cite{proposal:privacy-security-trust-cloud}.

To begin with, cloud technology is not currently as private as some organizations would like:
\begin{itemize}
\item \textit{User Data Control} --- In virtually any given Software-as-a-Service (SaaS) scenario, user data controls are sadly lacking.  Once data has been committed to a specific provider, that data is completely out of the original data owners control.  Furthermore, as we will see below, that data my not even be solely owned by the original owner anymore either.
\item \textit{Secondary Use} --- Most consumer facing social systems extensively mine user provided data for additional business advantages.  This is a common and well known secondary use for supplied data.  SaaS providers again have strong incentives to examine user provided information.
\item \textit{Offshore Development} --- Service users have no real control over who actually develops the systems a given service deploys.  Organizations have attempted to contractually limit development and support functions companies pursue to, say, the continental United States but have had very poor results with these kinds of unsupportable arrangements.
\item \textit{Data Routing} --- System providers in fact have little control over routing issues as well as system users.  Prohibiting data routing through sensitive countries is a difficult task for a single organization.
\item \textit{Secondary Storage} --- Most large-scale systems expect to use Content Delivery Networks (CDNs) to help manage content, and that expectation is heavily reflected in their physical system architectures. They simply cannot divorce use of CDNs from their systems for a single organization.
\item \textit{Bankruptcy and Data Ownership} --- Ownership and obligation to maintain expected data arrangements for a given company is unestablished under bankruptcy \cite{proposal:borders-info-I,proposal:borders-info-II,proposal:borders-info-III}.
\end{itemize}

Security issues also emerge from utility computing infrastructures:
\begin{itemize}
\item \textit{Data Access} --- System users have very little control over who, in the system provider's organization, is able to access their data and systems.
\item \textit{Data Deletion} --- Most savvy organizations have procedures in place to sanitize old storage elements like disk drives or backup tapes.  System users have very little control over if and how this is done when computing services are treated as a utility.
\item \textit{Backup Data Storage} --- Backup media is very difficult to encrypt, and most system providers still use tape systems as preferred media solutions for backup and storage needs.  These tapes, or copies of them, are generally stored offsite to support disaster recovery scenarios.  Security of these types of systems has been spotty to date \cite{proposal:saic-breach-I,proposal:saic-breach-II,proposal:saic-breach-III}.
\item \textit{Intercloud Standardization} --- Cloud computing systems do not have any standardized way to transfer computational units or data between systems.  Any protocols used for this kind of thing must be developed by customers themselves.  Due to the desire of providers to lock-in customers, this will likely not change as any standard development is strongly counter-incentiveized. 
\item \textit{Multi-tenancy and Side-Channels} --- Multi-tenant architectures in which multiple customers simultaneously use the same systems open those customers to covert side-channel attacks.
\item \textit{Logging and Auditing} --- Logging and auditing structures, especially for inter-cloud systems, are non-existent.
\end{itemize}

Finally, such systems suffer from internal and external trust issues:
\begin{itemize}
\item \textit{Trust Relationships} --- Trust is difficult to establish between individual cloud providers long-term.
\item \textit{Consumer Trust} --- Service users are still not entirely trusting of cloud system providers.
\end{itemize}

How to address these issues is an open research question.  Organizations ranging from cloud service providers to the military are exploring how to engineer solutions to these problems, and to more clearly understand the trade-offs required between selected system architectures \cite{proposal:assured-info-sharing}.  The problems themselves are wide ranging, appearing in a variety of different systems.  Military and other government systems are clearly impacted by these kinds of trust and security issues, and they also have clear information sensitivity semantics.  This, coupled with the fact that these organizations have been dealing with these issues in one form or another for decades make them very well suited for prototypical implementation and study.

The current standard in place dealing with these issues in this environment is managed by the Unified Cross Domain Management Office (UCDMO).  UCDMO stakeholders range from the DoD to the NSA.  The current standard in place and governed by the UCDMO to deal with this kinds of issues are \textit{guard-centric cross domain architectures}.

\subsection{Current Solutions}
Current and near-future proposed solutions endorsed by the UCDMO include system architectures assembled by the NSA, Raytheon, and Booz | Allen | Hamilton (BAH).   The NSA has been active in this area for decades as a logical extension of their role in signals intelligence collection and processing.  Raytheon and BAH have been engaged over the past few years to provide an alternative voice and design approach to these kinds of systems, an effort met with limited success.

These cross-domain solutions are intended to enable sensitive information to easily flow both from a higher sensitivity domain to a lower sensitivity domain, and from lower to higher as well.  They generally act over both primary data (say, a document) and metadata over that primary data as well.  Note that in these system, in most cases, human intervention is still required to adequately review data prior to passing into lower security domains.

\subsubsection{NSA, Filtered}
The NSA conducted initial work in this area.  Their standard-setting efforts culminated in a reasonable conceptual system architecture, using groups of filters dedicated to specific delineated tasks to process sensitive information. \cite{proposal:nsa-arch}.

\begin{figure}[!t]
\centering
\includegraphics[width=5in]{nsa-legacy-arch}
\caption{NSA Legacy Notional Architecture Model}
\label{fig:model:conceptual-model}
\end{figure}

In the scenario portrayed in figure \ref{fig:model:conceptual-model}, \textit{Domain A} could very well be a private cloud managed by the U.S. Air Force, while \textit{Domain B} is a public operational network of some kind shared by coalition partners in a joint operation.

A system user attempts to send a \textit{data package} consisting of a primary document and associated metadata from \textit{Domain A} to \textit{Domain B}.  At some point, that submission reaches a \textit{guard}, which contains at least one \textit{filter chain}.  Each filter chain then contains at least one \textit{filter}.  Individual filters can execute arbitrary actions over a submitted data package and have access to any number of external resources as required.  At any point, a filter can examine the data package and reject it, at which point it will frequently wait for human review.  If a filter does not reject a data package, it passes that package onto the next filter or submits it for delivery to Domain B.

\subsubsection{NSA, Services}
In recent years, the NSA has extended the legacy system architecture for cross-domain information sharing to exploit service-oriented computing styles \cite{proposal:nsa-arch}.  Visualized in \ref{fig:model:conceptual-model-services}, this model incorporates more modern conceptual elements and componentry.

\begin{figure}[!t]
\centering
\includegraphics[width=5in]{nsa-arch}
\caption{NSA Service-Oriented Model}
\label{fig:model:conceptual-model-services}
\end{figure}

In the view in Figure \ref{fig:model:conceptual-model-services}, we see on the left the \textit{Global Information Grid}, or \textit{GIG}.  On the right, we have the \textit{Distributed Service-oriented Cross Domain Solution}, or \textit{DSCDS}.  The GIG is not a truly open system --- rather, it is a loosely coupled collection of computational services handing data at a variety of levels of sensitivity, federated to provide stakeholder timely access to relevant information \cite{proposal:gig-arch}.  The DSCDS is essentially the embodiment of the NSA's cross-domain vision applied to service oriented computing.  This model fuses various technology choices with previous cross-domain thinking.

Indicative of this more modern system design thinking, we have a variety of services and service consumers attached to a common service bus within the GIG.  Within the DSCDS, we have groups of filters implemented as services inspecting transferred data when moved over the bus.  Finally, all of this interaction is managed by a management interface and controlled by an orchestration engine accessing a centralized group of policies.

Note that here we have begun to access a common policy repository for various types of security metadata regarding primary data elements.

\subsubsection{Raytheon}
In the past few years, Raytheon has offered a new model for cross domain use influenced by the NSA service-oriented model \cite{proposal:raytheon-arch}:

\begin{figure}[!t]
\centering
\includegraphics[width=5in]{raytheon-arch}
\caption{Ratheon Model}
\label{fig:model:conceptual-model-ray}
\end{figure}

The model in Figure \ref{fig:model:conceptual-model-ray}   is more grounded in the actual technical environment this kind of solution would be embedded within.  Here, we have the Non-secure Internet Protocol Router Network (NIPRNet) as one domain, and the Secret Internet Protocol Router Network (SIPRNet) as the other.  Here, NIPRNet is the lower security domain (lowside), and SIPRNet the higher security domain (highside).  This particular view shows the motion of data from the high side (SIPRNet) to the low side (NIPRNet).

Here, a data request is submitted from SIPRNet first two the \textit{XML Security Gateway} which calls into the \textit{Orchestration Engine} for policy validation.  The Orchstration Engine then coordinates calls into a \textit{Policy Repository} as well as to a collection of external \textit{Support Services}.  Once rectified against these elements, the request is passed into the \textit{Cross Domain Guard} which routes the request into the \textit{Unclassified Enclave} in NIPRNet.  Here, the request is passed directly through the lowside \textit{XML Security Gateway}, without rectification, onto the \textit{Service Provider}.  The response from the Service Provider is then passed back to the requester via the inverse path.

This model also begins to use a centralized policy repository, just as the NSA Service Model.  It also uses a single cross domain guard to transfer information from both the highside to the lowside, and vice-versa.

\subsubsection{Booz | Allen | Hamilton}
BAH submitted a competing model, also in 2009 \cite{proposal:bah-arch}.  In fact, both Raytheon and BAH presented their models under competitive contract to the UCDMO at the same conference, so the domain application is not coincidental.

\begin{figure}[!t]
\centering
\includegraphics[width=5in]{bah-arch}
\caption{Booz | Allen | Hamilton Model}
\label{fig:model:conceptual-model-bah}
\end{figure}

Figure \ref{fig:model:conceptual-model-bah} embodies BAH's thinking with respect to cross domain information management.  Note here, we have a \textit{Domain A} as a high security domain, and \textit{Domain B} as a low security domain.  Here, we again have dataflow from the highside to the lowside through the cross domain management system.

While not as detailed as the Raytheon proposal, this does have similar elements.  Here, we data first travels from Domain A into the \textit{Interface Segment for Domain A}, similar to the secret enclave used in the Raytheon model  From there, it moves into the \textit{CI Segment}, which in turn submits the transferring ata into the \textit{Filter Segment}.  From there, the package is moved into the \textit{Interface Segment for Domain B}, and then onto \textit{Domain B}.  The \textit{Administrative Segment} provides managment and oversight of the system as a whole.

Note the absence of specific policy-centric elements.  This system is reliant on specific filters as well.

\subsubsection{Shortcomings of Current Systems}
Having reviewed the current state of the art of these kinds of cross domain solutions,  they still have clear similarities, and in fact have not progressed far beyond the initial notions of how these kinds of systems should work.  The still, for example, all use some kind of filter chaining mechanism to evaluate whether a given data item can be moved from a classified to an unclassified network.  Both NSA models used filters explicitly, as did the BAH model.  They all use a single guard as well, a sole point of security and enforcement, providing perimeter data security, but nothing else.  In each of these current system architectures, users are only allowed to exchange one type of information per domain.  The physical instantiations of these models are locked by operational policy to a single classification level limit.  Users cannot, for example, have Top Secret material on a network accredited for Secret material.  Finally, these models violate the end-to-end principle in large service network design, centralizing intelligence rather than pushing that intelligence down to the ends of the system \cite{Blumenthal:2001:RDI:383034.383037}.

\subsubsection{Characteristics of Future Systems}
Future systems on the whole will demonstrate decentralized policy management capabilities, infrastructural reuse, the ability to integrate with cloud systems, and security in depth.  Policy management is decentralized and integrated within the fabric of the system.  The system is both more secure and resilient as a result, better able to control information and operate under stressful conditions.  Multi-tenancy can lower costs and increase reliability and is furthermore a common attribute of cloud systems.  An appropriately secured system facilitates integration of computing resources into multi-tenant environments.  The ability to handle multi-tenant environments and to reliably secure both data at rest and data in motion leads to computational environments deployable in cloud systems.  Finally, systems must operate under \textit{all} conditions, including when they are under attack or compromise \cite{proposal:ron-ross}.  Ergo, they must provide protection to sensitive data in depth.

\subsection{Related Work}

\section{Proposed Taxonomy}
A clear taxonomic organization of potential steps in approaching finer grained policy based usage management helps in describing the difficulties inherent in developing potential solutions as well as aiding in planning system evolution over time. Here, we have five distinct types of integrated policy-centric usage management systems, as shown in Figure \ref{fig:model:taxonomy}.  Of these five, only the first two levels are represented in current system application.

\begin{figure}[!t]
\centering
\includegraphics[width=5in]{blank}
\caption{Proposed Usage Management Taxonomy}
\label{fig:model:taxonomy}
\end{figure}

In this taxonomy, it is not required that systems pass through lower levels to reach higher ones.  This taxonomy represents a continuum of integration of usage management controls.  Systems can very well be designed to fit into higher taxonomic categories without addressing lower categories.  That said however, many of the supporting infrastructural services, like identification managment or logging and tracing systems, are common between multiple levels.

The taxonomy itself starts with the current state, integrating policy evaluation systems into the network fabric gradually, moving away from filters, then by adding policy evaluation into the routing fabric, then the computational nodes, and finally by incorporating evaluation directly into content.

\subsection{$\phi$ --- Single Guard, No Policies}
The $\phi$ classification consists of systems like the initial NSA and BAH notional models in Figures \ref{fig:model:conceptual-model} and \ref{fig:model:conceptual-model-bah}.

\begin{figure}[!t]
\centering
\includegraphics[width=5in]{blank}
\caption{Taxonomy ($\phi$)}
\label{fig:model:taxonomy-phi}
\end{figure}

These systems consist of two distinct domains, separated by a filter-centric single guard.  The initial NSA system model is clearly of this type, separating two domains with a guard using filter chains.  The BAH model is also of this type, using a Filter Segment to evaluate data packages transmitted between interface segments attached to specific domains.

Generally one of the domains supports more sensitive information than the other, but that is not always the case.  In the models we have examined this has certainly been true, but classified information for example  is commonly stored in \textit{compartments} which are separated by clear \textit{need-to-know} policies generally enforced by access lists and classification guides.  These kinds of compartments contain information at similar levels of classification, but contain distinct informational elements that should not be combined.

\subsection{$\alpha$ --- Single Guard, Policy Integration}

%\begin{figure}[!t]
%\centering
%\includegraphics[width=5in]{blank}
%\caption{Taxonomy ($\alpha$)}
%\label{fig:model:taxonomy-alpha}
%\end{figure}

\subsection{$\beta$ --- Router Guards}

%\begin{figure}[!t]
%\centering
%\includegraphics[width=5in]{blank}
%\caption{Taxonomy ($\beta$)}
%\label{fig:model:taxonomy-beta}
%\end{figure}

\subsection{$\gamma$ ---  Router and Node Guards}

%\begin{figure}[!t]
%\centering
%\includegraphics[width=5in]{blank}
%\caption{Taxonomy ($\gamma$)}
%\label{fig:model:taxonomy-gamma}
%\end{figure}

\subsection{$\delta$ --- Continuous Evaluation}

%\begin{figure}[!t]
%\centering
%\includegraphics[width=5in]{blank}
%\caption{Taxonomy ($\delta$)}
%\label{fig:model:taxonomy-delta}
%\end{figure}

\section{System Architectural Implications}

%\begin{figure}[!t]
%\centering
%\includegraphics[width=5in]{blank}
%\caption{Supporting Services}
%\label{fig:model:suppporting-elements}
%\end{figure}

\section{Scope and Contribution of Work}
The unique contribution of this work is a quantitative analysis of policy-centric overlay network options, associated taxonomies of use, and prototypical technology proofs-of-concept.
\begin{itemize}
\item \textit{Overlay Options} --- This includes various types of overlay networks and associated strengths and weaknesses addressing centralized and decentralized models
\item \textit{Taxonomies of Use} --- Depending on the specific usage management requirements and context, different overlays have different applicability; this work will provide guidance on suitability
\item \textit{Prototypical Technologies} --- Examples and proofs-of-concept will be required to appropriately analyze various architectural alternatives
\end{itemize}
This work will focus on the $\alpha$, $\beta$, and $\gamma$ taxonomic categories, excluding $\delta$ and $\phi$ classed systems.

\section{Conclusions}

\bibliographystyle{plain}
\bibliography{bib/proposal}

\end{document}